{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6zkUnBMIcJxbRGzpI2OE1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "ZBLotXB86-en"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text from the file\n",
        "with open('dataset.txt', 'r', encoding='utf-8') as file:\n",
        "    faqs = file.read()"
      ],
      "metadata": {
        "id": "hMc78fyh_p15"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([faqs])\n",
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmaXsJ137AII",
        "outputId": "0941b38f-5413-4ff6-be5e-dd7c8bc79ba3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1088"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer.word_index"
      ],
      "metadata": {
        "id": "NeJV4xu27luw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for sentence in faqs.split('\\n'):\n",
        "   tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "for i in range(1, len(tokenized_sentence)):\n",
        "    n_gram_sequence = tokenized_sentence[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "IdRJkTkD8W2v"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97CkQUXH-8Ef",
        "outputId": "2925fb57-b25d-4932-cc0f-b3e18f6f9b3a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[284, 195], [284, 195, 380], [284, 195, 380, 1088]]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Q54_FZYC1wH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}